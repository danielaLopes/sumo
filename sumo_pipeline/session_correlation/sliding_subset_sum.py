import numpy as np
import pickle
from typing import List, Dict, Tuple, Literal
import sys
import os
from tqdm import tqdm
from functools import  wraps, cached_property
import pyopencl as cl # catch openCL exceptions
from abc import ABC, abstractmethod
import logging
logging.basicConfig(level=logging.INFO)

import feature_collection
import process_results
import subsetSumOpenCL2DWrapper as sliding_subset_sum
import dataset_concurrency_analysis
import PerformanceMetrics
import flows

import results_plot_maker
import results_plot_maker_by_client
import results_plot_maker_by_os
import results_plot_maker_full_pipeline
import results_plot_maker_partial_coverage

from constants import *

np.set_printoptions(threshold=sys.maxsize)
np.set_printoptions(suppress=True)


class Scores:
    # [(window1, score1), ... (windown, scoren)]
    scores_per_window: List[Tuple[int, int]] # each position pertains to the score in the respective window

    def __init__(self):
        self.scores_per_window = []

    def add_score(self, score):
        self.scores_per_window.append(score)

class Prediction:
    final_score: float
    label: Literal[0, 1]

    def __init__(self, final_score, label):
        self.final_score = final_score
        self.label = label

    def __repr__(self):
        return f"Final score: {self.final_score}; label: {self.label}"

class SlidingSubsetSum:
    # Only serves as a placeholder for the class State
    pass

class State(ABC):
    """
    Base abstract state class to share functionality. 
    Implementation of the State Method Design Pattern
    """
    sss: SlidingSubsetSum

    epoch_size: int # seconds
    epoch_tolerance: int # epochs
    time_sampling_interval: int # ms
    window_size: int # windows
    overlap: int # windows
    delta: int 

    buckets_per_window: int
    buckets_overlap: int

    possible_request_combinations: Dict[Tuple[str, str], flows.FlowPair] # {(client_session_id, onion_session_id): FlowPair, ...}
    client_flows: Dict[str, flows.ClientFlow]
    onion_flows: Dict[str, flows.OnionFlow]

    # Calculate additional false negatives generated by mispredictions in the filtering phase
    # These flows did not reach the correlation stage when they should have
    missed_client_flows: int
    missed_os_flows: int
    missed_client_flows_per_duration: dict[int, int]
    missed_os_flows_per_duration: dict[int, int]

    database: Dict[Tuple[str, str], Scores] # (client_session_id, onion_session_id): Scores
    predictions: dict[float, dict[Tuple[str, str], Prediction]] # threshold : {client_session_id, onion_session_id): Prediction, ...}

    metrics_map: dict[float, PerformanceMetrics.PerformanceMetrics] # threshold: PerformanceMetrics
    metrics_map_final_scores: dict[float, PerformanceMetrics.PerformanceMetrics] # threshold: PerformanceMetrics
    scores_per_session_per_client: dict[str, dict] # client_session_id: {onion_session_id: score, ...}
    metrics_map_final_scores_per_session: dict[float, dict[str, PerformanceMetrics.PerformanceMetrics]] # threshold: {onion_session_id: PerformanceMetrics, ...}

    results_by_min_duration: dict[float, dict] # threshold: {min_session_duration: PerformanceMetrics, ...}
    results_by_min_duration_metrics_map: dict[float, dict] # threshold: {min_session_duration: PerformanceMetrics, ...}
    flow_count_by_duration: dict[int, int] # min_session_duration: flows_per_duration
    flow_count_by_duration_correlated: dict[int, int] # min_session_duration: flows_per_duration

    metrics_map_per_client: dict[float, dict] # threshold: {client_name: PerformanceMetrics, ...}

    metrics_map_per_onion: dict[float, dict] # threshold: {onion_name: PerformanceMetrics, ...}
    fp_sessions: dict[float, dict] # threshold: {onion_session_id: list, ...}

    def __init__(self, 
                 sss: SlidingSubsetSum,
                 epoch_size: int, 
                 epoch_tolerance: int, 
                 time_sampling_interval: int, 
                 window_size: int, 
                 overlap: int, 
                 delta: int):
        self.sss = sss

        self.epoch_size = epoch_size
        self.epoch_tolerance = epoch_tolerance
        self.time_sampling_interval = time_sampling_interval
        self.window_size = window_size
        self.overlap = overlap
        self.delta = delta
 
        self.buckets_per_window = int(window_size)
        self.buckets_overlap = int(overlap)

        self.possible_request_combinations = {}
        self.client_flows = {}
        self.onion_flows = {}

        self.missed_client_flows = 0
        self.missed_os_flows = 0
        self.missed_client_flows_per_duration = {}
        self.missed_os_flows_per_duration = {}

        self.database = {}
        self.predictions = {}

        self.metrics_map = {}
        self.metrics_map_final_scores = {}
        self.scores_per_session_per_client = {}
        self.metrics_map_final_scores_per_session = {}

        self.results_by_min_duration = {}
        self.results_by_min_duration_metrics_map = {}
        self.flow_count_by_duration = {}
        self.flow_count_by_duration_correlated = {}

        self.metrics_map_per_client = {}

        self.metrics_map_per_onion = {}
        self.fp_sessions = {}

    @abstractmethod
    def toggle_state(self) -> None:
        pass

    def plot(self) -> None:
        if not os.path.isdir(RESULTS_FOLDER):
            os.mkdir(RESULTS_FOLDER)
        if not os.path.isdir(FIGURES_RESULTS_FOLDER):
            os.mkdir(FIGURES_RESULTS_FOLDER)
        if not os.path.isdir(FIGURES_PAPER_RESULTS_FOLDER):
            os.mkdir(FIGURES_PAPER_RESULTS_FOLDER)
        if not os.path.isdir(FIGURES_FULL_PIPELINE_RESULTS_FOLDER):
            os.mkdir(FIGURES_FULL_PIPELINE_RESULTS_FOLDER)

class PreProcessedNoFullPipelineState(State):
    def __init__(self, 
                 sss: SlidingSubsetSum,
                 epoch_size: int, 
                 epoch_tolerance: int, 
                 time_sampling_interval: int, 
                 window_size: int, 
                 overlap: int, 
                 delta: int):
        super().__init__(sss,
                         epoch_size, 
                         epoch_tolerance, 
                         time_sampling_interval, 
                         window_size, 
                         overlap, 
                         delta)

    def toggle_state(self) -> None:
        self.sss.state = self.sss.pre_processed_no_full_pipeline_state

    def pre_process(self) -> None:
        (
            self.possible_request_combinations, 
            self.client_flows, 
            self.onion_flows
        ) = feature_collection.process_features_epochs_sessions(self.sss.dataset_name, 
                                                                self.time_sampling_interval,
                                                                self.buckets_per_window, 
                                                                self.buckets_overlap,
                                                                self.epoch_size, 
                                                                self.epoch_tolerance)

    def __repr__(self) -> str:
        return "PreProcessedNoFullPipelineState"
    
    def get_concurrency_at_onion(self, dataset_name: str):
        concurrency_file = get_session_concurrency_at_onion_file_name(dataset_name)
        if os.path.isfile(concurrency_file):
            concurrent_requests = pickle.load(open(concurrency_file, 'rb'))
        else:
            concurrent_requests = dataset_concurrency_analysis.get_session_concurrency_at_onions_from_features(dataset_name)
        return concurrent_requests
    
    def plot_paper_results(self, captures_folder_test: str, dataset_name: str, threshold: float) -> None:
        super().plot()

        concurrent_requests = self.get_concurrency_at_onion(dataset_name)
        results_plot_maker_by_os.plot_session_concurrency_per_os(FIGURES_PAPER_RESULTS_FOLDER, self.metrics_map_final_scores_per_session, concurrent_requests, dataset_name, threshold=threshold, plot_triple=False)
        
        # fps_oses_heatmap.pdf
        results_plot_maker_by_os.plot_heatmap_fps_per_oses(FIGURES_PAPER_RESULTS_FOLDER, self.metrics_map_per_onion, self.fp_sessions, dataset_name, threshold=threshold)

        # precision_recall_curve_with_threshold_multiple_session_durations.pdf
        results_plot_maker.precision_recall_curve_with_threshold_multiple_session_durations(FIGURES_PAPER_RESULTS_FOLDER, self.results_by_min_duration, self.flow_count_by_duration_correlated, dataset_name) 

        # session_results_statistics_cdfs.pdf
        results_plot_maker.session_results_statistics(FIGURES_PAPER_RESULTS_FOLDER, self.metrics_map_final_scores_per_session, captures_folder_test, dataset_name, threshold=threshold)

    def plot(self, captures_folder_test: str, dataset_name: str, threshold: float) -> None:
        super().plot()

        results_plot_maker_by_client.plot_triple_bars(FIGURES_RESULTS_FOLDER, self.metrics_map_per_client, dataset_name, threshold=threshold)

        concurrent_requests = self.get_concurrency_at_onion(dataset_name)
        results_plot_maker_by_os.plot_session_concurrency_per_os(FIGURES_RESULTS_FOLDER, self.metrics_map_final_scores_per_session, concurrent_requests, dataset_name, threshold=threshold)

        # # fps_oses_heatmap.pdf
        results_plot_maker_by_os.plot_heatmap_fps_per_oses(FIGURES_RESULTS_FOLDER, self.metrics_map_per_onion, self.fp_sessions, dataset_name, threshold=threshold)

        # # precision_recall_curve_with_threshold_multiple_session_durations.pdf
        results_plot_maker.precision_recall_curve_with_threshold_multiple_session_durations(FIGURES_RESULTS_FOLDER, self.results_by_min_duration, self.flow_count_by_duration_correlated, dataset_name)    
        
        # # Plot precision and recall lines with threshold on x-axis
        results_plot_maker.precision_recall_variation_with_threshold(FIGURES_RESULTS_FOLDER, self.metrics_map_final_scores, dataset_name)

        # session_results_statistics_cdfs.pdf
        results_plot_maker.session_results_statistics(FIGURES_RESULTS_FOLDER, self.metrics_map_final_scores_per_session, captures_folder_test, dataset_name, threshold=threshold)
        
        # session_statistics_cdfs.pdf
        results_plot_maker.session_dataset_statistics(FIGURES_RESULTS_FOLDER, captures_folder_test, dataset_name)

class PreProcessedFullPipelineState(State):
    def __init__(self, 
                 sss: SlidingSubsetSum,
                 epoch_size: int, 
                 epoch_tolerance: int, 
                 time_sampling_interval: int, 
                 window_size: int, 
                 overlap: int, 
                 delta: int):
        super().__init__(sss,
                         epoch_size, 
                         epoch_tolerance, 
                         time_sampling_interval, 
                         window_size, 
                         overlap, 
                         delta)

    def toggle_state(self) -> None:
        self.sss.state = self.sss.pre_processed_full_pipeline_state

    def pre_process(self) -> None:
        (
            self.possible_request_combinations,
            self.client_flows,
            self.onion_flows,
            self.missed_client_flows,
            self.missed_os_flows,
            self.missed_client_flows_per_duration,
            self.missed_os_flows_per_duration
        ) = feature_collection.process_features_epochs_sessions_full_pipeline(self.sss.dataset_name, 
                                                                              self.time_sampling_interval,
                                                                              self.buckets_per_window, 
                                                                              self.buckets_overlap, 
                                                                              self.epoch_size, 
                                                                              self.epoch_tolerance, 
                                                                              self.sss.min_session_durations)

    def __repr__(self) -> str:
        return "PreProcessedFullPipelineState"
    
    def plot(self) -> None:
        super().plot()
        results_plot_maker.precision_recall_curve_with_threshold_multiple_session_durations(FIGURES_FULL_PIPELINE_RESULTS_FOLDER, self.results_by_min_duration, self.flow_count_by_duration_correlated, self.sss.dataset_name)
    
class PreProcessedPartialCoveragePercentageState(State):
    coverage: float
    results_by_eu_country: Dict[float, PerformanceMetrics.PerformanceMetrics] # {coverage_percentage: PerformanceMetrics, ...}

    def __init__(self, 
                 sss: SlidingSubsetSum, 
                 epoch_size: int, 
                 epoch_tolerance: int, 
                 time_sampling_interval: int, 
                 window_size: int, 
                 overlap: int, 
                 delta: int,
                 coverage: float):
        super().__init__(sss,
                         epoch_size, 
                         epoch_tolerance, 
                         time_sampling_interval, 
                         window_size, 
                         overlap, 
                         delta)
        self.coverage = coverage

    #def toggle_state(self, coverage) -> None:
        #self.sss.state = self.sss.pre_processed_partial_coverage_percentage_states_by_coverage[coverage]
    def toggle_state(self) -> None:
        self.sss.state = self

    def pre_process(self) -> None:
        (
            self.possible_request_combinations, 
            self.client_flows, 
            self.onion_flows
        ) = feature_collection.process_features_epochs_sessions_by_eu_country(self.sss.dataset_name, 
                                                                                  self.coverage, 
                                                                                  self.time_sampling_interval, 
                                                                                  self.buckets_per_window, 
                                                                                  self.buckets_overlap,
                                                                                  self.epoch_size, 
                                                                                  self.epoch_tolerance)

    def __repr__(self) -> str:
        return "PreProcessedPartialCoveragePercentageState"
    
    #def plot(self, captures_folder_test: str, dataset_name: str, threshold: float) -> None:
    #    pass

class PreProcessedPartialCoverageStateByContinent(State):
    zone: str
    results_by_continent: Dict[str, PerformanceMetrics.PerformanceMetrics] # {zone: PerformanceMetrics, ...}

    def __init__(self, 
                 sss: SlidingSubsetSum, 
                 epoch_size: int, 
                 epoch_tolerance: int, 
                 time_sampling_interval: int, 
                 window_size: int, 
                 overlap: int, 
                 delta: int,
                 zone: str):
        super().__init__(sss,
                         epoch_size, 
                         epoch_tolerance, 
                         time_sampling_interval, 
                         window_size, 
                         overlap, 
                         delta)
        self.zone = zone

    #def toggle_state(self, coverage) -> None:
        #self.sss.state = self.sss.pre_processed_partial_coverage_percentage_states_by_coverage[coverage]
    def toggle_state(self) -> None:
        self.sss.state = self

    def pre_process(self) -> None:
        (
            self.possible_request_combinations, 
            self.client_flows, 
            self.onion_flows
        ) = feature_collection.process_features_epochs_sessions(self.sss.dataset_name, 
                                                                self.time_sampling_interval,
                                                                self.buckets_per_window, 
                                                                self.buckets_overlap,
                                                                self.epoch_size, 
                                                                self.epoch_tolerance)

    def __repr__(self) -> str:
        return "PreProcessedPartialCoverageStateByContinent"
    
    #def plot(self, captures_folder_test: str, dataset_name: str, threshold: float) -> None:
    #    pass

    def filter_by_zone(self, baseline_possible_request_combinations):
        for start, (client_session_id, onion_session_id) in enumerate(self.baseline_possible_request_combinations.keys()):
            splitclient_session_id = client_session_id.split('_')
            splitonion_session_id = onion_session_id.split('_')
            client_name1 = splitclient_session_id[0].split('-client')[0]
            os_name1 = 'os-' + splitclient_session_id[1].split('-os-')[1]
            client_name2 = splitonion_session_id[0].split('-client')[0]
            os_name2 = 'os-' + splitonion_session_id[1].split('-os-')[1]
    
            if not (self.zone == self.sss.client_zones[client_name1] or self.zone == self.os_zones[os_name2]):
                self.possible_request_combinations[(client_session_id, onion_session_id)] = baseline_possible_request_combinations[(client_session_id, onion_session_id)]

def get_cache_filename(dataset_name):
    return f"{DATA_RESULTS_FOLDER}sliding_subset_sum_{dataset_name}.pickle"

def dump_instance_decorator(arg_index):
    """
    Decorates functions so that the instance of SlidingSubsetSum
    gets stored at the end of each main function
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            result = func(*args, **kwargs)
            
            # Pickle dump the instance
            instance = args[0]  # Assuming the instance is the first argument
            dataset_name = args[arg_index]
            cached_filename = get_cache_filename(dataset_name)
            logging.info(f"Storing file named \"{cached_filename}\"")
            with open(cached_filename, "wb") as file:
                pickle.dump(instance, file)

            return result
        return wrapper
    return decorator

class SlidingSubsetSum:
    dataset_name: str 

    min_session_durations: list[float]
    thresholds: list[int]

    # State Method Design Pattern
    state: State
    pre_processed_no_full_pipeline_state: PreProcessedNoFullPipelineState
    pre_processed_full_pipeline_state: PreProcessedFullPipelineState
    pre_processed_partial_coverage_percentage_states_by_coverage: Dict[float, PreProcessedPartialCoveragePercentageState]

    # TODO: make alternate __init__ function that starts hyperparameters with hyperparameter tuning
    @dump_instance_decorator(arg_index=1)
    def __init__(self, 
                 dataset_name,
                 epoch_size, 
                 epoch_tolerance, 
                 time_sampling_interval, 
                 window_size, 
                 overlap, 
                 delta,
                 epoch_size_full_pipeline, 
                 epoch_tolerance_full_pipeline, 
                 time_sampling_interval_full_pipeline, 
                 window_size_full_pipeline, 
                 overlap_full_pipeline, 
                 delta_full_pipeline) -> None:
        self.dataset_name = dataset_name

        self.min_session_durations = np.arange(0, 25, 1)
        self.min_session_durations *= 60 # minutes to seconds
        self.thresholds = np.arange(-0.3, 0.3, 0.001)

        self.pre_processed_no_full_pipeline_state = PreProcessedNoFullPipelineState(self,
                                                                                    epoch_size, 
                                                                                    epoch_tolerance, 
                                                                                    time_sampling_interval, 
                                                                                    window_size, 
                                                                                    overlap, 
                                                                                    delta)
        self.pre_processed_full_pipeline_state = PreProcessedFullPipelineState(self, 
                                                                                epoch_size_full_pipeline, 
                                                                                epoch_tolerance_full_pipeline, 
                                                                                time_sampling_interval_full_pipeline, 
                                                                                window_size_full_pipeline, 
                                                                                overlap_full_pipeline, 
                                                                                delta_full_pipeline)

        self.coverage_percentages = {
                                'Full coverage': 1,
                                'Germany': 0.3354,
                                'Germany + France': 0.4850,
                                'Germany + France + Netherlands': 0.5573,
                                'All EU countries': 0.7648
                            }
        self.pre_processed_partial_coverage_percentage_states_by_coverage = {}
        for coverage in self.coverage_percentages.values():
            self.pre_processed_partial_coverage_percentage_states_by_coverage[coverage] = PreProcessedPartialCoveragePercentageState(self, 
                                                                                                                                     epoch_size, 
                                                                                                                                     epoch_tolerance, 
                                                                                                                                     time_sampling_interval, 
                                                                                                                                     window_size, 
                                                                                                                                     overlap, 
                                                                                                                                     delta,
                                                                                                                                     coverage)

        # TODO: get this dynamically from ansible inventory
        self.client_zones = {'client-small-ostest-1': 'europe',
                'client-small-ostest-2': 'australia',
                'client-small-ostest-3': 'america',    
                'client-small-ostest-4': 'asia',
                'client-small-ostest-5': 'america'
                }
        self.os_zones = {'os-small-ostest-1': 'europe',
                'os-small-ostest-2': 'europe',
                'os-small-ostest-3': 'europe',    
                'os-small-ostest-4': 'america'
                }
        self.zones = ['baseline', 'europe', 'america', 'asia', 'australia']

        self.pre_processed_partial_coverage_states_by_continent = {}
        for zone in self.zones:
            self.pre_processed_partial_coverage_states_by_continent[zone] = PreProcessedPartialCoverageStateByContinent(self, 
                                                                                                                        epoch_size, 
                                                                                                                        epoch_tolerance, 
                                                                                                                        time_sampling_interval, 
                                                                                                                        window_size, 
                                                                                                                        overlap, 
                                                                                                                        delta,
                                                                                                                        zone)

        # Default state
        self.state = self.pre_processed_no_full_pipeline_state

    @cached_property
    def get_zones_without_baseline(self):
        return self.zones[1 : ]

    @cached_property
    def cached_filename(self):
        return get_cache_filename(self.dataset_name)

    @classmethod
    def get_instance(cls: SlidingSubsetSum, 
                        dataset_name: str,
                        epoch_size: int, 
                        epoch_tolerance: int, 
                        time_sampling_interval: int, 
                        window_size: int, 
                        overlap: int, 
                        delta: int,
                        epoch_size_full_pipeline: int, 
                        epoch_tolerance_full_pipeline: int, 
                        time_sampling_interval_full_pipeline: int, 
                        window_size_full_pipeline: int, 
                        overlap_full_pipeline: int, 
                        delta_full_pipeline: int) -> SlidingSubsetSum:
        cached_filename = get_cache_filename(dataset_name)
        try:
            # Try to load the cached instance
            with open(cached_filename, 'rb') as cache_file:
                instance = pickle.load(cache_file)
                logging.info(f"Loaded existing instance named \"{cached_filename}\"")
        except FileNotFoundError:
            logging.info(f"Creating new instance named \"{cached_filename}\"")
            instance = cls(dataset_name,
                            epoch_size, 
                            epoch_tolerance, 
                            time_sampling_interval, 
                            window_size, 
                            overlap, 
                            delta,
                            epoch_size_full_pipeline, 
                            epoch_tolerance_full_pipeline, 
                            time_sampling_interval_full_pipeline, 
                            window_size_full_pipeline, 
                            overlap_full_pipeline, 
                            delta_full_pipeline)
        logging.info(f"Instance: {instance}")
        return instance

    def __repr__(self):
        return f"""Sliding Subset Sum ({self.dataset_name})\n
                    * Baseline ({self.pre_processed_no_full_pipeline_state}) params => epoch_size: {self.pre_processed_no_full_pipeline_state.epoch_size};
                    epoch_tolerance: {self.pre_processed_no_full_pipeline_state.epoch_tolerance};
                    time_sampling_interval: {self.pre_processed_no_full_pipeline_state.time_sampling_interval};
                    window_size: {self.pre_processed_no_full_pipeline_state.window_size};
                    overlap: {self.pre_processed_no_full_pipeline_state.overlap};
                    delta: {self.pre_processed_no_full_pipeline_state.delta}\n
                    * Full pipeline ({self.pre_processed_full_pipeline_state}) params: => epoch_size: {self.pre_processed_full_pipeline_state.epoch_size};
                    epoch_tolerance: {self.pre_processed_full_pipeline_state.epoch_tolerance};
                    time_sampling_interval: {self.pre_processed_full_pipeline_state.time_sampling_interval};
                    window_size: {self.pre_processed_full_pipeline_state.window_size};
                    overlap: {self.pre_processed_full_pipeline_state.overlap};
                    delta: {self.pre_processed_full_pipeline_state.delta}\n
                    current state: {self.state}\n
                    # pairs of flows: {len(self.state.possible_request_combinations)}\n
                    """
                    # # client flows: {len(self.state.client_flows)}\n
                    # # onion flows: {len(self.state.onion_flows)}\n
                    
    
    @dump_instance_decorator(arg_index=1)
    def __pre_process(self, dataset_name, is_full_pipeline: bool) -> None:
        if is_full_pipeline == False:
            self.pre_processed_no_full_pipeline_state.toggle_state()
        else:
            self.pre_processed_full_pipeline_state.toggle_state()
        logging.info(f"Toggled state {self.state}")

        if len(self.state.possible_request_combinations) == 0:
            self.state.pre_process()

    def __run_windowed_subset_sum_on_all_pairs(self):
        packet_list_clients = []
        packet_list_onions = []
        n_buckets = []
        n_windows = []
        count_pairs = 0
        keys = list(self.state.possible_request_combinations.keys())

        for client_session_id, onion_session_id in tqdm(keys, desc="Applying subset sum on possibly correlated pairs ..."): 
            # We don't even have enough data for a full window
            if self.state.possible_request_combinations[(client_session_id, onion_session_id)].session_buckets < self.state.buckets_per_window:
                self.state.possible_request_combinations.pop((client_session_id, onion_session_id))
                continue
            if self.state.possible_request_combinations[(client_session_id, onion_session_id)].session_windows == 0:
                self.state.possible_request_combinations.pop((client_session_id, onion_session_id))
                continue
            
            packet_list_clients += list(self.state.possible_request_combinations[(client_session_id, onion_session_id)].bucketized_client_packets_in)
            packet_list_onions += list(self.state.possible_request_combinations[(client_session_id, onion_session_id)].bucketized_onion_packets_out)

            n_buckets.append(self.state.possible_request_combinations[(client_session_id, onion_session_id)].session_buckets)
            n_windows.append(self.state.possible_request_combinations[(client_session_id, onion_session_id)].session_windows)
            count_pairs += 1

        acc = 0
        acc_windows = [acc]
        for i in range(1, len(n_windows)):
            acc += n_windows[i - 1]
            acc_windows.append(acc)

        scores = sliding_subset_sum.whole_loop_subset_sum(packet_list_clients, packet_list_onions, count_pairs, n_buckets, self.state.delta, self.state.buckets_per_window, self.state.buckets_overlap, n_windows, acc_windows)

        for start, (client_session_id, onion_session_id) in tqdm(enumerate(self.state.possible_request_combinations.keys()), desc="Obtaining correlation scores ..."):
            if start >= len(acc_windows):
                continue
            for j in range(0, self.state.possible_request_combinations[(client_session_id, onion_session_id)].session_windows):
                key = (client_session_id, onion_session_id)
                if key not in self.state.database:
                    self.state.database[key] = Scores()
                self.state.database[key].add_score((j, scores[acc_windows[start] + j]))

    def __calculate_final_score(self, client_session_id: str, onion_session_id: str) -> float:
        seq_penalty = 0
        seq_gain = 0
        score = 0
        for window, score in self.state.database[(client_session_id, onion_session_id)].scores_per_window:
            if score == 1:
                seq_penalty = 0
                score += (1 + 0.1 * seq_gain)
                seq_gain += 1

            elif score == -1:
                seq_gain = 0
                score -= (1 + 0.1 * seq_penalty)
                seq_penalty += 1

        return score / self.state.possible_request_combinations[(client_session_id, onion_session_id)].session_windows
    
    @dump_instance_decorator(arg_index=1)
    def __predict(self, dataset_name) -> None:
        if len(self.state.database) == 0:
            self.__run_windowed_subset_sum_on_all_pairs()
        else:
            logging.info("Already had data on database")

        if len(self.state.predictions) == 0:
            keys = list(self.state.possible_request_combinations.keys())
            for threshold in self.thresholds:
                logging.info(f"Evaluating threshold {threshold}")
                self.state.predictions[threshold] = {}
                for client_session_id, onion_session_id in keys: 
                    try:
                        final_score = self.__calculate_final_score(client_session_id, onion_session_id)
                    except KeyError as e:
                        if (client_session_id, onion_session_id) in self.state.possible_request_combinations:
                            self.state.possible_request_combinations.pop((client_session_id, onion_session_id))
                        continue
                    if final_score >= threshold:
                        # predicted as correlated
                        self.state.predictions[threshold][(client_session_id, onion_session_id)] = Prediction(final_score=final_score, label=1)
                    else:
                        # predicted as not correlated
                        self.state.predictions[threshold][(client_session_id, onion_session_id)] = Prediction(final_score=final_score, label=0)
        else:
            logging.info("Already had data on predictions")

    @dump_instance_decorator(arg_index=1)
    def __evaluate_confusion_matrix(self, dataset_name) -> None:
        if len(self.state.metrics_map) == 0:
            for i, (threshold, pair_preds) in enumerate(self.state.predictions.items()):
                #print("Evaluating threshold {}".format(threshold))
                self.state.metrics_map[threshold] = PerformanceMetrics.PerformanceMetrics(self.state.missed_client_flows, self.state.missed_os_flows)
                self.state.metrics_map_final_scores[threshold] = PerformanceMetrics.PerformanceMetrics(self.state.missed_client_flows, self.state.missed_os_flows)
                self.state.metrics_map_final_scores_per_session[threshold] = {}

                for (client_session_id, onion_session_id), label_score in pair_preds.items(): 
                    if i == 0:     
                        if client_session_id not in self.state.scores_per_session_per_client:
                            self.state.scores_per_session_per_client[client_session_id] = {}
                        self.state.scores_per_session_per_client[client_session_id][onion_session_id] = label_score.final_score
                    
                    if label_score.label == 1:
                        if client_session_id == onion_session_id:
                            self.state.metrics_map[threshold].tp += 1
                        else:
                            self.state.metrics_map[threshold].fp += 1
                            
                    else:
                        if client_session_id == onion_session_id:
                            self.state.metrics_map[threshold].fn += 1
                        else:
                            self.state.metrics_map[threshold].tn += 1

                self.state.metrics_map[threshold].calculate_performance_scores()

                client_sessions_with_highest_scores = process_results.count_client_correlated_sessions_highest_score(self.state.scores_per_session_per_client, threshold)
                for client_session_id, onion_session_id in self.state.possible_request_combinations.keys():
                    if onion_session_id not in self.state.metrics_map_final_scores_per_session[threshold]:
                        self.state.metrics_map_final_scores_per_session[threshold][onion_session_id] = PerformanceMetrics.PerformanceMetrics(missed_client_flows_full_pipeline=0, missed_os_flows_full_pipeline=0)
                    if client_session_id == onion_session_id:
                        if client_sessions_with_highest_scores[client_session_id]['correlatedHighestScore']:
                            self.state.metrics_map_final_scores[threshold].tp += 1
                            self.state.metrics_map_final_scores_per_session[threshold][onion_session_id].tp += 1
                        else:
                            self.state.metrics_map_final_scores_per_session[threshold][onion_session_id].fn += 1
                            self.state.metrics_map_final_scores[threshold].fn += 1
                    else:
                        if client_sessions_with_highest_scores[client_session_id]['falseHighestScore'] and client_sessions_with_highest_scores[client_session_id]['falseSession'] == onion_session_id:
                            self.state.metrics_map_final_scores_per_session[threshold][onion_session_id].fp += 1
                            self.state.metrics_map_final_scores[threshold].fp += 1
                        else:
                            self.state.metrics_map_final_scores_per_session[threshold][onion_session_id].tn += 1
                            self.state.metrics_map_final_scores[threshold].tn += 1
                
                for performance_metrics in self.state.metrics_map_final_scores_per_session[threshold].values():
                    performance_metrics.calculate_performance_scores()
                self.state.metrics_map_final_scores[threshold].calculate_performance_scores()
            
        else:
            logging.info("Already had data on evaluate_confusion_matrix()")

    @dump_instance_decorator(arg_index=1)
    def __evaluate_by_duration(self, dataset_name, is_full_pipeline=False) -> None:
        if len(self.state.results_by_min_duration) == 0:
            for threshold in self.thresholds:
                self.state.results_by_min_duration[threshold] = {}
                self.state.results_by_min_duration_metrics_map[threshold] = {}
                client_sessions_with_highest_scores = process_results.count_client_correlated_sessions_highest_score(self.state.scores_per_session_per_client, threshold)
                for min_session_duration in self.min_session_durations:
                    flows_per_duration = 0
                    flows_per_duration_correlated = 0
                    if is_full_pipeline == False:
                        self.state.results_by_min_duration[threshold][min_session_duration] = PerformanceMetrics.PerformanceMetrics(self.state.missed_client_flows, self.state.missed_os_flows)
                        self.state.results_by_min_duration_metrics_map[threshold][min_session_duration] = PerformanceMetrics.PerformanceMetrics(self.state.missed_client_flows, self.state.missed_os_flows)
                    else:
                        self.state.results_by_min_duration[threshold][min_session_duration] = PerformanceMetrics.PerformanceMetrics(self.state.missed_client_flows, self.state.missed_os_flows)
                        self.state.results_by_min_duration_metrics_map[threshold][min_session_duration] = PerformanceMetrics.PerformanceMetrics(self.state.missed_client_flows, self.state.missed_os_flows)
                    for client_session_id, onion_session_id in self.state.possible_request_combinations.keys():
                        duration = self.state.client_flows[client_session_id].get_duration()

                        if duration < min_session_duration:
                            continue
                        flows_per_duration += 1

                        if client_session_id == onion_session_id:
                            flows_per_duration_correlated += 1
                            if client_sessions_with_highest_scores[client_session_id]['correlatedHighestScore']:
                                self.state.results_by_min_duration[threshold][min_session_duration].tp += 1
                            else:
                                self.state.results_by_min_duration[threshold][min_session_duration].fn += 1
                        else:
                            if client_sessions_with_highest_scores[client_session_id]['falseHighestScore'] and client_sessions_with_highest_scores[client_session_id]['falseSession'] == onion_session_id:
                                self.state.results_by_min_duration[threshold][min_session_duration].fp += 1
                            else:
                                self.state.results_by_min_duration[threshold][min_session_duration].tn += 1

                        if client_session_id == onion_session_id:
                            if self.state.scores_per_session_per_client[client_session_id][onion_session_id] > threshold:
                                self.state.results_by_min_duration_metrics_map[threshold][min_session_duration].tp += 1
                            else:
                                self.state.results_by_min_duration_metrics_map[threshold][min_session_duration].fn += 1
                        else:
                            if self.state.scores_per_session_per_client[client_session_id][onion_session_id] > threshold:
                                self.state.results_by_min_duration_metrics_map[threshold][min_session_duration].fp += 1
                            else:
                                self.state.results_by_min_duration_metrics_map[threshold][min_session_duration].tn += 1

                    self.state.flow_count_by_duration[min_session_duration] = flows_per_duration
                    self.state.flow_count_by_duration_correlated[min_session_duration] = flows_per_duration_correlated

            for threshold in self.thresholds:
                for min_session_duration in self.min_session_durations:
                    self.state.results_by_min_duration[threshold][min_session_duration].calculate_performance_scores()

                    self.state.results_by_min_duration_metrics_map[threshold][min_session_duration].calculate_performance_scores()
        else:
            logging.info("Already had data on evaluate_by_duration()")

    @dump_instance_decorator(arg_index=1)
    def __evaluate_by_client(self, dataset_name) -> None:
        if len(self.state.metrics_map_per_client) == 0:
            for threshold in self.thresholds:
                self.state.metrics_map_per_client[threshold] = {}
                client_sessions_with_highest_scores = process_results.count_client_correlated_sessions_highest_score(self.state.scores_per_session_per_client, threshold)
                for client_session_id, onion_session_id in self.state.possible_request_combinations.keys():
                    client_name = client_session_id.split("_")[0]
                    if client_name not in self.state.metrics_map_per_client[threshold]:
                        self.state.metrics_map_per_client[threshold][client_name] = PerformanceMetrics.PerformanceMetrics(missed_client_flows_full_pipeline=0, missed_os_flows_full_pipeline=0)

                    if client_session_id == onion_session_id:
                        if client_sessions_with_highest_scores[client_session_id]['correlatedHighestScore']:
                            self.state.metrics_map_per_client[threshold][client_name].tp += 1
                        else:
                            self.state.metrics_map_per_client[threshold][client_name].fn += 1
                    else:
                        if client_sessions_with_highest_scores[client_session_id]['falseHighestScore'] and client_sessions_with_highest_scores[client_session_id]['falseSession'] == onion_session_id:
                            self.state.metrics_map_per_client[threshold][client_name].fp += 1
                        else:
                            self.state.metrics_map_per_client[threshold][client_name].tn += 1

            for threshold in self.thresholds:
                for client_name in self.state.metrics_map_per_client[threshold].keys():
                    self.state.metrics_map_per_client[threshold][client_name].calculate_performance_scores()
        else:
            logging.info("Already had data on evaluate_by_client()")

    @dump_instance_decorator(arg_index=1)
    def __evaluate_by_os(self, dataset_name) -> None:
        if len(self.state.metrics_map_per_onion) == 0:
            for threshold in self.thresholds:
                self.state.metrics_map_per_onion[threshold] = {}
                self.state.fp_sessions[threshold] = {}
                client_sessions_with_highest_scores = process_results.count_client_correlated_sessions_highest_score(self.state.scores_per_session_per_client, threshold)
                for client_session_id, onion_session_id in self.state.possible_request_combinations.keys():
                    onion_name = onion_session_id.split("_")[1] # TODO: have an overall method for this
                    if onion_name not in self.state.metrics_map_per_onion[threshold]:
                        self.state.metrics_map_per_onion[threshold][onion_name] = PerformanceMetrics.PerformanceMetrics(missed_client_flows_full_pipeline=0, missed_os_flows_full_pipeline=0)

                    if client_session_id == onion_session_id:
                        if client_sessions_with_highest_scores[client_session_id]['correlatedHighestScore']:
                            self.state.metrics_map_per_onion[threshold][onion_name].tp += 1
                        else:
                            self.state.metrics_map_per_onion[threshold][onion_name].fn += 1
                    
                    else:
                        if client_sessions_with_highest_scores[client_session_id]['falseHighestScore'] and client_sessions_with_highest_scores[client_session_id]['falseSession'] == onion_session_id:
                            self.state.metrics_map_per_onion[threshold][onion_name].fp += 1
                            if onion_session_id not in self.state.fp_sessions[threshold]:
                                self.state.fp_sessions[threshold][onion_session_id] = []
                            self.state.fp_sessions[threshold][onion_session_id].append(client_session_id)  
                        else:
                            self.state.metrics_map_per_onion[threshold][onion_name].tn += 1

            for threshold in self.thresholds:
                for onion_name in self.state.metrics_map_per_onion[threshold].keys():
                    self.state.metrics_map_per_onion[threshold][onion_name].calculate_performance_scores()
        else:
            logging.info("Already had data on evaluate_by_os()")

    def correlate_sessions(self, dataset_name, is_full_pipeline=False) -> None:
        if not os.path.isdir(RESULTS_FOLDER):
            os.mkdir(RESULTS_FOLDER)
        if not os.path.isdir(DATA_RESULTS_FOLDER):
            os.mkdir(DATA_RESULTS_FOLDER)

        self.__pre_process(dataset_name, is_full_pipeline)
        logging.info("\n--- After pre_process")
        self.__predict(dataset_name)
        logging.info("\n--- After predict")
        
        self.__evaluate_confusion_matrix(dataset_name)
        logging.info("\n--- After evaluate_confusion_matrix")
        self.__evaluate_by_duration(dataset_name, is_full_pipeline=is_full_pipeline)
        logging.info("\n--- After evaluate_by_duration")
        self.__evaluate_by_client(dataset_name)
        logging.info("\n--- After evaluate_by_client")
        self.__evaluate_by_os(dataset_name)
        logging.info("\n--- After evaluate_by_os")

    def plot_paper_results(self, captures_folder_test: str, dataset_name: str) -> None:
        chosen_threshold = 0
        self.pre_processed_no_full_pipeline_state.toggle_state()
        self.state.plot_paper_results(captures_folder_test, dataset_name, self.thresholds[chosen_threshold])

    def plot(self, captures_folder_test: str, dataset_name: str) -> None:
        chosen_threshold = 0
        self.pre_processed_no_full_pipeline_state.toggle_state()
        self.state.plot(captures_folder_test, dataset_name, self.thresholds[chosen_threshold])

    def plot_full_pipeline(self) -> None:
        self.pre_processed_full_pipeline_state.toggle_state()
        self.state.plot()

    @dump_instance_decorator(arg_index=1)
    def evaluate_coverage_by_eu_country(self, dataset_name: str):
        for zone, percentage in self.coverage_percentages.items():
            #self.pre_processed_partial_coverage_percentage_states_by_coverage[coverage].toggle_state(coverage)
            self.pre_processed_partial_coverage_percentage_states_by_coverage[percentage].toggle_state()
            if len(self.state.possible_request_combinations) == 0:
                self.state.pre_process()
            if len(self.state.results_by_eu_country) == 0:
                #for zone, percentage in self.coverage_percentage.items():
                logging.info(f"\n====== ANALYZING ZONE: {zone}")
                self.__predict(dataset_name)
                self.__evaluate_confusion_matrix(self, dataset_name)

                for threshold in self.state.results_by_zone[percentage].keys():
                    self.state.results_by_zone[percentage][threshold].calculate_performance_scores()

        # Group results from all states
        results_by_eu_country = {}
        for zone, percentage in self.coverage_percentages.items():
            results_by_eu_country[percentage] = self.state.results_by_zone
        self.pre_processed_partial_coverage_percentage_states_by_coverage[1].plot()
        results_plot_maker_partial_coverage.precision_recall_curve_with_threshold_by_eu_country(self.results_by_eu_country, self.coverage_percentage.values(), dataset_name)

    @dump_instance_decorator(arg_index=1)
    def evaluate_coverage_by_continent(self, dataset_name: str):
        self.pre_processed_partial_coverage_states_by_continent['baseline'].toggle_state()
        if len(self.state.possible_request_combinations) == 0:
            self.state.pre_process()
        for zone in self.get_zones_without_baseline():
            self.pre_processed_partial_coverage_states_by_continent[zone].toggle_state()
            self.state.possible_request_combinations = self.state.filter_by_zone(self.pre_processed_partial_coverage_states_by_continent['baseline'].possible_request_combinations)
        for zone in self.zones:
            if len(self.state.results_by_continent) == 0:
                logging.info(f"\n====== ANALYZING ZONE: {zone}")
                self.__predict(dataset_name)
                self.__evaluate_confusion_matrix(self, dataset_name)

                for threshold in self.state.results_by_continent.keys():
                    self.state.results_by_continent[threshold].calculate_performance_scores()

        # Group results from all states
        results_by_continent = {}
        for zone in self.zones:
            results_by_continent[zone] = self.state.results_by_continent
        self.pre_processed_partial_coverage_states_by_continent['baseline'].plot()
        results_plot_maker_partial_coverage.precision_recall_curve_with_threshold_excluding_zones(results_by_continent, dataset_name)